<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<!--  <link rel="icon" href="assets/xhr.ico" type="image/x-icon">-->
  <link rel="apple-touch-icon" sizes="180x180" href="assets/favicon_package/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="assets/jinliang.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="assets/jinliang.jpg">
  <link rel="manifest" href="assets/favicon_package/site.webmanifest">
  <link rel="mask-icon" href="assets/favicon_package/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="preload" href="assets/fonts/xxx.woff" as="font" type="font/woff" crossorigin>

  <title>Jinliang Zheng</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="main.css">
  <link rel="canonical" href="2toinf.github.io">
  <link rel="preload" href="assets/font/Mukta-Light.ttf" as="font" type="font/woff" crossorigin>
  <link rel="preload" href="assets/font/Mukta-Medium.ttf" as="font" type="font/woff" crossorigin>

  <!-- <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Mukta:wght@300;400&display=swap" rel="stylesheet"> -->
  <!-- <style>
    a { color: #FF0000; } /* CSS link color */
  </style> -->
</head>

<body data-new-gr-c-s-check-loaded="14.1029.0" data-gr-ext-installed="">

  <header class="site-header" role="banner">
    <div class="wrapper navigation-wrapper ">
      <div class="navigation-links">
        <span class="site-title">Jinliang Zheng (<span style="font-family:'Kaiti SC'">ÈÉëÈáë‰∫Æ</span>)</span>
      </div>
    </div>
  </header>

  <main class="page-content" aria-label="Content">
    <div class="wrapper">

      <article class="post">
        <div class="post-content">
          <img src="assets/jinliang.jpg" class="profile-picture" >
          <p><br></p>

          <p>
             Hi! I'm a second-year PhD student at AIR, 
             Tsinghua University, studying embodied AI. I am fortunately 
             advised by Prof.<a href ="http://zhanxianyuan.xyz/"> Xianyuan Zhan</a> and 
             Prof. <a href="https://air.tsinghua.edu.cn/en/info/1046/1188.htm">Ya-Qin Zhang</a>.   
             I previously gained valuable experience in Computer Vision through an internship at SenseTime Research. 
             I'm driven by the challenge of building truly embodied intelligence 
             and am committed to pushing the boundaries of this field. I'm still on the road!
          </p>
          <p> I am open to collaboration, feel free to reach me out!</p>


          <p>
            <a href="https://github.com/2toinf">
              <svg role="img" width="20" height="20" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <title>GitHub</title>
                <path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/>
              </svg></i>Github</span></a> 
            <a href="https://x.com/2_toinf">
              <svg role="img" width="20" height="20" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>X</title><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"/></svg>
              Twitter</a> /
            <a href="https://scholar.google.com/citations?user=3j5AHFsAAAAJ&hl=zh-CN">
              <svg role="img" width="20" height="20" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <title>Google Scholar</title>
                <path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z"/>
              </svg>
              <span>Google Scholar</span>
            </a> /
            <a href="li-jx21@mails.tsinghua.edu.cn">
              <svg role="img" width="20" height="20" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Gmail</title><path d="M24 5.457v13.909c0 .904-.732 1.636-1.636 1.636h-3.819V11.73L12 16.64l-6.545-4.91v9.273H1.636A1.636 1.636 0 0 1 0 19.366V5.457c0-2.023 2.309-3.178 3.927-1.964L5.455 4.64 12 9.548l6.545-4.91 1.528-1.145C21.69 2.28 24 3.434 24 5.457z"/></svg>
              zhengjl23@mails.tsinghua.edu.cn</a>
          </p>

          <!-- <strong>I am actively looking for postdoc and full research positions. Weclome to drop me an email if interested.</strong> -->

        </div>
      </article>
    </div>

    <div class="wrapper">
      <!-- <article class="post"> -->
        <header class="post-header">
          <h1 class="post-title">News </h1>
        </header>
        <article class="post">
          <ul>
            <li>One paper (<a href="https://arxiv.org/abs/2501.10105">UniAct</a>) on cross-embodiment universal actions is accepted to <strong>CVPR 2025</strong>. </li>
            <li>üåü<a href="https://zhengyinan-air.github.io/Diffusion-Planner/">Diffusion-Planner</a> is selected as <strong>oral</strong> presentation at <strong>ICLR 2025</strong>.</li>
            <li>One papers on autonomous driving (<a href="https://zhengyinan-air.github.io/Diffusion-Planner/">Diffusion-Planner</a>) are accepted to <strong>ICLR 2025</strong>. </li>
            <li>I finished my internship at SenseTime Research and was lucky to join OpenRobotLab at Shanghai Artificial Intelligence Laboratory. Looking forward to exploring the world of robotics!</li>
            <li>One paper (<a href="https://zh1hao.wang/Robo_MUTUAL/">Robo-MUTUAL</a>) on embodied representations is accepted to <strong>ICRA 2025</strong>. </li>
            <li>üåü<a href="https://2toinf.github.io/IVM/">IVM</a> and <a href="https://2toinf.github.io/DecisionNCE/">DecisionNCE</a> are selected as <strong>Outstanding Paper</strong> at MFM-EAI workshop @ <strong>ICML 2024</strong>.</li>
            <li>One paper (<a href="https://2toinf.github.io/IVM/">IVM</a>) on embodied foundation multimodal models is accepted to <strong>NeurIPS 2024</strong>.</li>
            <li>One paper (<a href="https://2toinf.github.io/DecisionNCE/">DecisionNCE</a>) on embodied multimodal representations is accepted to <strong>ICML 2024</strong>.</li>
            <!-- <li>Honored to be selected as <a href="https://neurips.cc/Conferences/2022/ProgramCommittee">Top Reviewers in NeurIPS 2022</a>. </li> -->
            <li>üåüOne paper (<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_GLID_Pre-training_a_Generalist_Encoder-Decoder_Vision_Model_CVPR_2024_paper.pdf">GLID</a>) on unified vision pretraining is accepted to <strong>CVPR 2024 </strong>. </li>
          </ul>
        </article>
    </div>

    <div class="wrapper">
      <article class="post">
        <header class="post-header">
          <h1 class="post-title" id="Publications">Publications <span style="font-size:small;letter-spacing:0.00px;">(* marks equal contribution)</a></span> </h1>
          <!-- <span class="footnote">* marks equal contribution</span> -->
        </header> 
      </article>
    <!-- Tab links -->
    <div class="tab">
      <button class="tablinks" onclick="openCity(event, 'Selected')" id="defaultOpen">üåü Selected</button>
      <button class="tablinks" onclick="openCity(event, 'All')">üåê All</button>
      <!-- <button class="tablinks" onclick="openCity(event, 'Efficient Pretrain')">üéÇ Efficient Pretrain</button>
     <button class="tablinks" onclick="openCity(event, 'Fast Post-train')">üî® Fast Post-train</button>
     <button class="tablinks" onclick="openCity(event, 'RL + X')">üìö RL + X</button> -->
    </div>

    <!-- Tab content -->
    <div id="Selected" class="tabcontent">
      <!-- <h3>Selected</h3> -->
      <ul class="publications">
        <li class="article">
          <span class="title">
            Universal Actions for Enhanced Embodied Foundation Models
          </span>
              <span class="authors"><strong>Jinliang Zheng*</strong>, Jianxiong Li*, Dongxiu Liu*, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan</span>
              <span class="journal-info">CVPR 2025</span>
              <!-- <span class="oral">(Spotlight, Top 5%)</span> -->
              <span class="year">2025</span>
              <span class="links">
            <a href="https://arxiv.org/abs/2501.10105">Paper</a> |
            <a href="https://github.com/2toinf/UniAct?tab=readme-ov-file">Code</a> |
            <a href="https://2toinf.github.io/UniAct/">Page</a>
          </span>
          </li>

          <li class="article">
            <span class="title">
              Instruction Guided Visual Masking
            </span>
                <span class="authors"><strong>Jinliang Zheng*</strong>, Jianxiong Li*, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, Xianyuan Zhan</span>
                <span class="journal-info">NeurIPS 2024</span>
                <span class="oral">(Outstanding Paper @ ICML 2024 MFM-EAI Workshop)</span>
                <span class="year">2024</span>
                <span class="links">
              <a href="https://arxiv.org/abs/2405.19783">Paper</a> |
              <a href="https://github.com/2toinf/IVM">Code</a> |
              <a href="https://2toinf.github.io/IVM/">Page</a> |
              <a href="https://huggingface.co/datasets/2toINF/IVM-Mix-1M/tree/main">Dataset</a> |
              <a href="https://huggingface.co/2toINF/IVM/tree/main">Model</a>
            </span>
            </li>

            <li class="article">
              <span class="title">
                GLID: Pre-training a Generalist Encoder-Decoder Vision Model
              </span>
                  <span class="authors">Jihao Liu*, <strong>Jinliang Zheng*</strong>,  Yu Liu, Hongsheng Li</span>
                  <span class="journal-info">CVPR 2024</span>
                  <span class="year">2024</span>
                  <span class="links">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_GLID_Pre-training_a_Generalist_Encoder-Decoder_Vision_Model_CVPR_2024_paper.pdf">Paper</a> |
                <!-- <a href="https://huggingface.co/datasets/2toINF/IVM-Mix-1M/tree/main">Dataset</a> |
                <a href="https://huggingface.co/2toINF/IVM/tree/main">Model</a> -->
              </span>
              </li>


              <li class="article">
                <span class="title">
                  Robo-MUTUAL: Robotic Multimodal Task Specification via Unimodal Learning
                </span>
                    <span class="authors">Jianxiong Li*, Zhihao Wang*, <strong>Jinliang Zheng*</strong>, Xiaoai Zhou, Guanming Wang, Guanglu Song, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Junzhi Yu, Xianyuan Zhan</span>
                    <span class="journal-info">ICRA 2025</span>
                    <!-- <span class="oral">(Spotlight, Top 5%)</span> -->
                    <span class="year">2025</span>
                    <span class="links">
                  <a href="https://arxiv.org/pdf/2410.01529">Paper</a> |
                  <a href="https://github.com/255isWhite/Robo_MUTUAL">Code</a> |
                  <a href="https://zh1hao.wang/Robo_MUTUAL/">Page</a>
                  <!-- <a href="https://huggingface.co/datasets/LTL07/PSEC">Dataset</a> | -->
                  <!-- <a href="https://huggingface.co/LTL07/PSEC">Model</a> -->
                </span>
                </li>
    </ul>

    </div>

    <div id="All" class="tabcontent">
      <ul class="publications">

        <li class="article">
          <span class="title">
            Universal Actions for Enhanced Embodied Foundation Models
          </span>
              <span class="authors"><strong>Jinliang Zheng*</strong>, Jianxiong Li*, Dongxiu Liu*, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan</span>
              <span class="journal-info">CVPR 2025</span>
              <!-- <span class="oral">(Spotlight, Top 5%)</span> -->
              <span class="year">2025</span>
              <span class="links">
            <a href="https://arxiv.org/abs/2501.10105">Paper</a> |
            <a href="https://github.com/2toinf/UniAct?tab=readme-ov-file">Code</a> |
            <a href="https://2toinf.github.io/UniAct/">Page</a>
          </span>
          </li>

          <li class="article">
            <span class="title">
              Instruction Guided Visual Masking
            </span>
                <span class="authors"><strong>Jinliang Zheng*</strong>, Jianxiong Li*, Sijie Cheng, Yinan Zheng, Jiaming Li, Jihao Liu, Yu Liu, Jingjing Liu, Xianyuan Zhan</span>
                <span class="journal-info">NeurIPS 2024</span>
                <span class="oral">(Outstanding Paper @ ICML 2024 MFM-EAI Workshop)</span>
                <span class="year">2024</span>
                <span class="links">
              <a href="https://arxiv.org/abs/2405.19783">Paper</a> |
              <a href="https://github.com/2toinf/IVM">Code</a> |
              <a href="https://2toinf.github.io/IVM/">Page</a> |
              <a href="https://huggingface.co/datasets/2toINF/IVM-Mix-1M/tree/main">Dataset</a> |
              <a href="https://huggingface.co/2toINF/IVM/tree/main">Model</a>
            </span>
            </li>

            <li class="article">
              <span class="title">
                DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning
              </span>
                  <span class="authors">Jianxiong Li*, <strong>Jinliang Zheng*</strong>, Yinan Zheng*, Liyuan Mao, Xiao Hu, Sijie Cheng, Haoyi Niu, Jihao Liu, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan</span>
                  <span class="journal-info">ICML 2024</span>
                  <span class="oral">(Outstanding Paper @ ICML 2024 MFM-EAI Workshop)</span>
                  <span class="year">2024</span>
                  <span class="links">
                <a href="https://arxiv.org/pdf/2402.18137.pdf">Paper</a> |
                <a href="https://github.com/2toinf/DecisionNCE">Code</a> |
                <a href="https://2toinf.github.io/DecisionNCE/">Page</a>
                <!-- <a href="https://huggingface.co/datasets/2toINF/IVM-Mix-1M/tree/main">Dataset</a> |
                <a href="https://huggingface.co/2toINF/IVM/tree/main">Model</a> -->
              </span>
              </li>
              <li class="article">
                <span class="title">
                  Robo-MUTUAL: Robotic Multimodal Task Specification via Unimodal Learning
                </span>
                    <span class="authors">Jianxiong Li*, Zhihao Wang*, <strong>Jinliang Zheng*</strong>, Xiaoai Zhou, Guanming Wang, Guanglu Song, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Junzhi Yu, Xianyuan Zhan</span>
                    <span class="journal-info">ICRA 2025</span>
                    <!-- <span class="oral">(Spotlight, Top 5%)</span> -->
                    <span class="year">2025</span>
                    <span class="links">
                  <a href="https://arxiv.org/pdf/2410.01529">Paper</a> |
                  <a href="https://github.com/255isWhite/Robo_MUTUAL">Code</a> |
                  <a href="https://zh1hao.wang/Robo_MUTUAL/">Page</a>
                  <!-- <a href="https://huggingface.co/datasets/LTL07/PSEC">Dataset</a> | -->
                  <!-- <a href="https://huggingface.co/LTL07/PSEC">Model</a> -->
                </span>
                </li>
          
            <li class="article">
                  <span class="title">
                    GLID: Pre-training a Generalist Encoder-Decoder Vision Model
                  </span>
                      <span class="authors">Jihao Liu*, <strong>Jinliang Zheng*</strong>,  Yu Liu, Hongsheng Li</span>
                      <span class="journal-info">CVPR 2024</span>
                      <span class="year">2024</span>
                      <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_GLID_Pre-training_a_Generalist_Encoder-Decoder_Vision_Model_CVPR_2024_paper.pdf">Paper</a> |
                    <!-- <a href="https://huggingface.co/datasets/2toINF/IVM-Mix-1M/tree/main">Dataset</a> |
                    <a href="https://huggingface.co/2toINF/IVM/tree/main">Model</a> -->
                  </span>
                  </li>
          <li class="article">
            <span class="title">
              Efficient Robotic Policy Learning via Latent Space Backward Planning
            </span>
                <span class="authors">Dongxiu Liu*, Haoyi Niu*, Zhihao Wang, <strong>Jinliang Zheng</strong>, Yinan Zheng, Zhonghong Ou, Jianming Hu, Jianxiong Li, Xianyuan Zhan</span>
                <span class="journal-info">Under Review</span>
                <!-- <span class="oral">(Spotlight, Top 5%)</span> -->
                <span class="year">2025</span>
                <span class="links">
              <!-- <a href="https://arxiv.org/abs/2501.10105">Paper</a> | -->
              <!-- <a href="https://github.com/2toinf/UniAct?tab=readme-ov-file">Code</a> | -->
              <!-- <a href="https://2toinf.github.io/UniAct/">Page</a> -->
            </span>
          </li>


          <li class="article">
            <span class="title">
              Diffusion-Based Planning for Autonomous Driving with Flexible Guidance
            </span>
                <span class="authors">Yinan Zheng*, Ruiming Liang*, Kexin Zheng*, <strong>Jinliang Zheng</strong>, Liyuan Mao, Jianxiong Li, Weihao Gu, Rui Ai, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu</span>
                <span class="journal-info">ICLR 2025</span>
                <span class="oral">(Oral, Top 2%)</span>
                <span class="year">2025</span>
                <span class="links">
              <a href="https://arxiv.org/pdf/2501.15564">Paper</a> |
              <a href="https://github.com/ZhengYinan-AIR/Diffusion-Planner">Code</a> |
              <a href="https://zhengyinan-air.github.io/Diffusion-Planner/">Page</a>
            </span>
          </li>

      </ul>
    </div>


    <!-- <div id="Efficient Pretrain" class="tabcontent">
      <ul class="publications">

      </ul>
    </div>

    <div id="Fast Post-train" class="tabcontent">
      <ul class="publications">

      </ul>
    </div>


    <div id="RL + X" class="tabcontent">
      <ul class="publications">

      </ul>
    </div>     -->


  </div>


      <!--    <div class="wrapper">-->
<!--      <article class="post">-->
<!--        <header class="post-header">-->
<!--        <h1 class="post-title">Awards</h1>-->
<!--        </header>-->
<!--        <div class="post-content">-->
<!--          <strong><a href="https://2021.ecmlpkdd.org/index.html@p=2148.html">Best ML Paper Award</a></strong> (1/685), ECML-PKDD, 2021 <br>-->
<!--          &lt;!&ndash; <a href="https://advml-workshop.github.io/icml2021/" style="color:#000000;"> &ndash;&gt;-->
<!--            <strong><a href="https://advml-workshop.github.io/icml2021/">Silver Best Paper Award</a></strong>, ICML AML workshop, 2021<br>-->
<!--          <strong>National Scholarship</strong>, Ministry of Education of China, 2021, 2022 <br>-->
<!--          <strong>Principal Scholarship</strong>, Peking University, 2022 <br>-->
<!--          <strong>Baidu Scholarship Nomination Award</strong> (20 worldwide), Baidu Inc, 2022 <br>-->
<!--      </div>-->
<!--    </div>-->

    <div class="wrapper">
      <article class="post">
        <header class="post-header">
        <h1 class="post-title">Professional Services</h1>
        </header>
        <div class="post-content">
        Reviewer for ICLR 25, ICML 25, NeurIPS 24-25, CVPR 25, ICCV 25
        </div>
      </article>
    </div>
    
    <div style="text-align:center">
    <a href="https://clustrmaps.com/site/1bwwr"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=1tWK5iJHiJKp5kb_J0EQjBE_l5jxdNI2bKf3LCzaK7E&cl=ffffff" /></a>
    </div>
    <!--  <div class="wrapper">-->
<!--    <article class="post">-->
<!--      <header class="post-header">-->
<!--      <h1 class="post-title">Teaching</h1>-->
<!--      </header>-->
<!--      <div class="post-content">-->
<!--      2022 Spring, TA in <strong>Advances in Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>-->
<!--      2021 Spring, TA in <strong>Trustworthy Machine Learning</strong>, instructed by Prof. Yisen Wang. <br>-->
<!--      2018 Spring, TA in <strong>Optimization in Machine Learning</strong>, instructed by Prof. Zhouchen Lin. <br>-->
<!--      2017 Fall, TA in <strong>Machine Learning</strong>, instructed by Prof. Tong Lin. <br>-->
<!--      </div>-->
<!--    </article>-->
<!--  </div>-->

<!--    <div class="wrapper">-->
<!--      <article class="post">-->
<!--        <header class="post-header">-->
<!--        <h1 class="post-title">Students Advised</h1>-->
<!--        </header>-->
<!--        <div class="post-content">-->
<!--            I have been lucky to co-advise a number of talented undergraduate and master‚Äôs students at Stanford, who have written some very insightful papers:-->

<!--            I have also mentored (and often proposed research directions) for a number of fantastic PhD students who have taught me a lot:-->

<!--        </div>-->
<!--      </article>-->
<!--    </div>-->

<!--      <div class="wrapper">-->
<!--          <article class="post">-->
<!--              <header class="post-header">-->
<!--                  <h1 class="post-title">Collaborators and Advisors</h1>-->
<!--              </header>-->
<!--              <div class="post-content">-->
<!--                  I spent a wonderful summer working with Suriya Gunasekar and Sebastien Bubeck in the Microsoft Research Foundations Group. During my PhD I‚Äôve also been lucky to collaborate with Aditi Raghunathan, Sang Michael Xie, Chelsea Finn, and Zico Kolter, and learn a lot from John Duchi. Before my PhD, I spent a fun year working at DeepMind, and before that did undergraduate research work with Avrim Blum, Guy Blelloch, and Bob Harper.-->
<!--              </div>-->
<!--          </article>-->
<!--      </div>-->





      <!--<script-->
<!--        type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=LISh56gffeAxOYBOk45sr6LRD7c60rWH1uPJOca6hLs&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'>-->
<!--</script>-->


<script>
  // Check if API exists
  if (document && document.fonts) {    
    // Do not block page loading
    setTimeout(function () {           
      document.fonts.load('16px "Mukta"').then(() => {
        // Make font using elements visible
        document.documentElement.classList.add('font-loaded') 
      })
    }, 0)
  } else {
    // Fallback if API does not exist 
    document.documentElement.classList.add('font-loaded') 
  }
</script>

<script>
  function openCity(evt, cityName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
      tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
      tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(cityName).style.display = "block";
    evt.currentTarget.className += " active";
  }
// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
  </script>

  </footer>

</body>
